extern crate reqwest;
extern crate scraper;
extern crate spider;

use reqwest::Error;
use scraper::{Html, Selector};
use spider::tokio;
use spider::website::Website;
use std::fs::write;

#[tokio::main]
async fn main() {
    println!("ðŸš€ Starting Web Crawler...");

    let sitemap_urls = vec![
        "https://www.heygoody.com/sitemap.xml",
        "https://www.heygoody.com/th/sitemap.xml",
        "https://www.heygoody.com/th/sitemap_index.xml",
        "https://www.heygoody.com/th/post-sitemap.xml",
        "https://www.heygoody.com/th/page-sitemap.xml",
    ];

    let mut all_urls = Vec::new();

    for sitemap in &sitemap_urls {
        if let Ok(content) = fetch_sitemap_raw(sitemap).await {
            if sitemap.contains("sitemap_index") {
                let sub_sitemaps = extract_sitemap_links(&content);
                for sub_sitemap in sub_sitemaps {
                    if let Ok(sub_content) = fetch_sitemap_raw(&sub_sitemap).await {
                        let urls = extract_urls_from_sitemap(&sub_content);
                        all_urls.extend(urls);
                    }
                }
            } else {
                let urls = extract_urls_from_sitemap(&content);
                all_urls.extend(urls);
            }
        }
    }

    if all_urls.is_empty() {
        println!("âš ï¸ No URLs found. Check the sitemap availability.");
        return;
    }

    println!("ðŸŒ Found {} URLs to process.", all_urls.len());

    for url in all_urls {
        if is_spa(&url).await {
            fetch_with_chrome(&url).await;
        } else {
            fetch_with_http(&url).await;
        }
    }

    println!("ðŸŽ‰ Web Crawling Completed!");
}

// âœ… à¹‚à¸«à¸¥à¸” Sitemap à¸”à¸´à¸š
async fn fetch_sitemap_raw(url: &str) -> Result<String, Error> {
    let response = reqwest::get(url).await?;
    response.text().await
}

// âœ… à¸”à¸¶à¸‡ Sitemap à¸¢à¹ˆà¸­à¸¢à¸ˆà¸²à¸ `sitemap index.xml`
fn extract_sitemap_links(xml: &str) -> Vec<String> {
    let document = Html::parse_document(xml);
    let selector = Selector::parse("loc").unwrap();
    document
        .select(&selector)
        .map(|node| node.text().collect::<String>())
        .collect()
}

// âœ… à¸”à¸¶à¸‡ URLs à¸ˆà¸²à¸ Sitemap
fn extract_urls_from_sitemap(xml: &str) -> Vec<String> {
    let document = Html::parse_document(xml);
    let selector = Selector::parse("url loc").unwrap();
    document
        .select(&selector)
        .map(|node| node.text().collect::<String>())
        .collect()
}

// âœ… à¹€à¸Šà¹‡à¸„à¸§à¹ˆà¸²à¹€à¸›à¹‡à¸™ SPA à¸«à¸£à¸·à¸­à¹„à¸¡à¹ˆ
async fn is_spa(url: &str) -> bool {
    let response = reqwest::get(url).await;
    match response {
        Ok(resp) => {
            let body = resp.text().await.unwrap_or_default();
            body.contains("window.__NUXT__") || body.contains("data-reactroot")
        }
        Err(_) => false,
    }
}

// âœ… Fetch à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸ˆà¸²à¸ SSR (HTTP Request)
async fn fetch_with_http(url: &str) {
    let response = reqwest::get(url).await;
    if let Ok(resp) = response {
        let html = resp.text().await.unwrap_or_default();
        let markdown = html_to_markdown(&html);
        let filename = format!("output_{}.md", sanitize_filename(url));
        write(&filename, markdown).expect("Failed to write file");
    }
}

// âœ… Fetch à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸ˆà¸²à¸ SPA (Chrome Headless)
async fn fetch_with_chrome(url: &str) {
    let mut website: Website = Website::new(url).with_caching(true).build().unwrap();
    website.crawl().await;

    if let Some(pages) = website.get_pages() {
        if let Some(page) = pages.iter().find(|p| p.get_url() == url) {
            let html = page.get_html();
            let markdown = html_to_markdown(&html);
            let filename = format!("output_{}.md", sanitize_filename(url));
            write(&filename, markdown).expect("Failed to write file");
        }
    }
}

// âœ… à¹à¸›à¸¥à¸‡ HTML â†’ Markdown
fn html_to_markdown(html: &str) -> String {
    let document = Html::parse_document(html);
    let selector = Selector::parse("body").unwrap();
    document
        .select(&selector)
        .next()
        .map(|body| body.text().collect::<Vec<_>>().join("\n"))
        .unwrap_or_else(|| "âš ï¸ No content extracted".to_string())
}

// âœ… à¸—à¸³à¹ƒà¸«à¹‰à¸Šà¸·à¹ˆà¸­à¹„à¸Ÿà¸¥à¹Œà¸›à¸¥à¸­à¸”à¸ à¸±à¸¢
fn sanitize_filename(url: &str) -> String {
    url.replace("https://", "")
        .replace("http://", "")
        .replace("/", "_")
        .replace(":", "_")
}
