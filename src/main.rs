extern crate reqwest;
extern crate scraper;
extern crate spider;

use reqwest::Error;
use scraper::{Html, Selector};
use spider::tokio;
use spider::website::Website;
use std::fs::{create_dir_all, write};
use std::time::Instant;

#[tokio::main]
async fn main() {
    println!("ðŸš€ Starting Web Crawler...");
    let start_time = Instant::now();

    let sitemap_urls = vec![
        "https://www.heygoody.com/sitemap.xml",
        "https://www.heygoody.com/th/sitemap.xml",
        "https://www.heygoody.com/th/sitemap_index.xml",
        "https://www.heygoody.com/th/post-sitemap.xml",
        "https://www.heygoody.com/th/page-sitemap.xml",
    ];

    let mut all_urls = Vec::new();
    let mut spa_urls = Vec::new();
    let mut ssr_urls = Vec::new();

    // âœ… 1. à¸”à¸¶à¸‡ URLs à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”à¸ˆà¸²à¸ Sitemap
    for sitemap in &sitemap_urls {
        if let Ok(content) = fetch_sitemap_raw(sitemap).await {
            if sitemap.contains("sitemap_index") {
                let sub_sitemaps = extract_sitemap_links(&content);
                for sub_sitemap in sub_sitemaps {
                    if let Ok(sub_content) = fetch_sitemap_raw(&sub_sitemap).await {
                        let urls = extract_urls_from_sitemap(&sub_content);
                        all_urls.extend(urls);
                    }
                }
            } else {
                let urls = extract_urls_from_sitemap(&content);
                all_urls.extend(urls);
            }
        }
    }

    if all_urls.is_empty() {
        println!("âš ï¸ No URLs found. Check the sitemap availability.");
        return;
    }

    println!("ðŸŒ Found {} URLs to process.", all_urls.len());

    // âœ… 2. à¹à¸¢à¸ URLs à¹€à¸›à¹‡à¸™ SPA à¹à¸¥à¸° SSR
    for url in all_urls.iter() {
        if is_spa(url).await {
            spa_urls.push(url.clone());
        } else {
            ssr_urls.push(url.clone());
        }
    }

    // âœ… 3. à¸ˆà¸±à¸”à¹€à¸à¹‡à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥ Markdown à¹€à¸›à¹‡à¸™à¸à¸¥à¸¸à¹ˆà¸¡à¹ƒà¸™ `/all-markdown/{category}/`
    for url in all_urls.iter() {
        let category = categorize_url(url);
        let dir_path = format!("all-markdown/{}", category);
        create_dir_all(&dir_path).expect("Failed to create directory");

        let filename = format!("{}/{}.md", dir_path, sanitize_filename(url));

        if spa_urls.contains(url) {
            fetch_with_chrome(url, &filename).await;
        } else {
            fetch_with_http(url, &filename).await;
        }
    }

    // âœ… 4. à¸ªà¸£à¹‰à¸²à¸‡à¹„à¸Ÿà¸¥à¹Œ `summary.txt`
    let elapsed_time = start_time.elapsed();
    let summary_content = format!(
        "ðŸŒ Total URLs: {}\nSPA URLs: {}\nSSR URLs: {}\nâ³ Total Crawl Time: {:.2?}",
        all_urls.len(),
        spa_urls.len(),
        ssr_urls.len(),
        elapsed_time
    );

    write("all-markdown/summary.txt", summary_content).expect("Failed to write summary file");

    println!("ðŸŽ‰ Web Crawling Completed!");
}

// âœ… à¹‚à¸«à¸¥à¸” Sitemap à¸”à¸´à¸š
async fn fetch_sitemap_raw(url: &str) -> Result<String, Error> {
    let response = reqwest::get(url).await?;
    response.text().await
}

// âœ… à¸”à¸¶à¸‡ Sitemap à¸¢à¹ˆà¸­à¸¢à¸ˆà¸²à¸ `sitemap index.xml`
fn extract_sitemap_links(xml: &str) -> Vec<String> {
    let document = Html::parse_document(xml);
    let selector = Selector::parse("loc").unwrap();
    document
        .select(&selector)
        .map(|node| node.text().collect::<String>())
        .collect()
}

// âœ… à¸”à¸¶à¸‡ URLs à¸ˆà¸²à¸ Sitemap
fn extract_urls_from_sitemap(xml: &str) -> Vec<String> {
    let document = Html::parse_document(xml);
    let selector = Selector::parse("url loc").unwrap();
    document
        .select(&selector)
        .map(|node| node.text().collect::<String>())
        .collect()
}

// âœ… à¹€à¸Šà¹‡à¸„à¸§à¹ˆà¸²à¹€à¸›à¹‡à¸™ SPA à¸«à¸£à¸·à¸­à¹„à¸¡à¹ˆ
async fn is_spa(url: &str) -> bool {
    let response = reqwest::get(url).await;
    match response {
        Ok(resp) => {
            let body = resp.text().await.unwrap_or_default();
            body.contains("window.__NUXT__") || body.contains("data-reactroot")
        }
        Err(_) => false,
    }
}

// âœ… Fetch à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸ˆà¸²à¸ SSR (HTTP Request)
async fn fetch_with_http(url: &str, filename: &str) {
    let response = reqwest::get(url).await;
    if let Ok(resp) = response {
        let html = resp.text().await.unwrap_or_default();
        let markdown = html_to_markdown(&html);
        write(filename, markdown).expect("Failed to write file");
    }
}

// âœ… Fetch à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸ˆà¸²à¸ SPA (Chrome Headless)
async fn fetch_with_chrome(url: &str, filename: &str) {
    let mut website: Website = Website::new(url).with_caching(true).build().unwrap();
    website.crawl().await;

    if let Some(pages) = website.get_pages() {
        if let Some(page) = pages.iter().find(|p| p.get_url() == url) {
            let html = page.get_html();
            let markdown = html_to_markdown(&html);
            write(filename, markdown).expect("Failed to write file");
        }
    }
}

// âœ… à¹à¸›à¸¥à¸‡ HTML â†’ Markdown
fn html_to_markdown(html: &str) -> String {
    let document = Html::parse_document(html);
    let selector = Selector::parse("body").unwrap();
    document
        .select(&selector)
        .next()
        .map(|body| body.text().collect::<Vec<_>>().join("\n"))
        .unwrap_or_else(|| "âš ï¸ No content extracted".to_string())
}

// âœ… à¸—à¸³à¹ƒà¸«à¹‰à¸Šà¸·à¹ˆà¸­à¹„à¸Ÿà¸¥à¹Œà¸›à¸¥à¸­à¸”à¸ à¸±à¸¢
fn sanitize_filename(url: &str) -> String {
    url.replace("https://", "")
        .replace("http://", "")
        .replace("/", "_")
        .replace(":", "_")
}

// âœ… à¸ˆà¸±à¸”à¸à¸¥à¸¸à¹ˆà¸¡ URL à¹€à¸›à¹‡à¸™ Directory (à¸•à¸²à¸¡à¸›à¸£à¸°à¹€à¸ à¸—à¸‚à¸­à¸‡ URL)
fn categorize_url(url: &str) -> String {
    if url.contains("/blog") {
        "blogs".to_string()
    } else if url.contains("/product") {
        "products".to_string()
    } else if url.contains("/news") {
        "news".to_string()
    } else {
        "others".to_string()
    }
}
