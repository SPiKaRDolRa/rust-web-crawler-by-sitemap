extern crate reqwest;
extern crate scraper;
extern crate spider;

use reqwest::Error;
use scraper::{Html, Selector};
use spider::tokio;
use spider::website::Website;
use std::fs::{create_dir_all, write};
use std::time::Instant;

#[tokio::main]
async fn main() {
    println!("üöÄ Starting Web Crawler...");
    let start_time = Instant::now();

    // ‚úÖ ‡∏£‡∏∞‡∏ö‡∏∏‡πÇ‡∏î‡πÄ‡∏°‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
    let domain = "https://www.heygoody.com";

    // ‚úÖ ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ Sitemap ‡∏î‡πâ‡∏ß‡∏¢‡∏´‡∏•‡∏≤‡∏¢‡∏ß‡∏¥‡∏ò‡∏µ
    let sitemap_urls = discover_sitemaps(domain).await;

    if sitemap_urls.is_empty() {
        println!("‚ö†Ô∏è No Sitemaps found for {}", domain);
        return;
    }

    let mut all_urls = Vec::new();
    let mut spa_urls = Vec::new();
    let mut ssr_urls = Vec::new();

    // ‚úÖ ‡∏î‡∏∂‡∏á URLs ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏à‡∏≤‡∏Å Sitemap
    for sitemap in &sitemap_urls {
        if let Ok(content) = fetch_sitemap_raw(sitemap).await {
            if sitemap.contains("sitemap_index") {
                let sub_sitemaps = extract_sitemap_links(&content);
                for sub_sitemap in sub_sitemaps {
                    if let Ok(sub_content) = fetch_sitemap_raw(&sub_sitemap).await {
                        let urls = extract_urls_from_sitemap(&sub_content);
                        all_urls.extend(urls);
                    }
                }
            } else {
                let urls = extract_urls_from_sitemap(&content);
                all_urls.extend(urls);
            }
        }
    }

    if all_urls.is_empty() {
        println!("‚ö†Ô∏è No URLs found from Sitemaps.");
        return;
    }

    println!("üåê Found {} URLs to process.", all_urls.len());

    // ‚úÖ ‡πÅ‡∏¢‡∏Å URLs ‡πÄ‡∏õ‡πá‡∏ô SPA ‡πÅ‡∏•‡∏∞ SSR
    for url in all_urls.iter() {
        if is_spa(url).await {
            spa_urls.push(url.clone());
        } else {
            ssr_urls.push(url.clone());
        }
    }

    // ‚úÖ ‡∏à‡∏±‡∏î‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Markdown ‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏•‡∏∏‡πà‡∏°‡πÉ‡∏ô `/all-markdown/{category}/`
    for url in all_urls.iter() {
        let category = categorize_url(url);
        let dir_path = format!("all-markdown/{}", category);
        create_dir_all(&dir_path).expect("Failed to create directory");

        let filename = format!("{}/{}.md", dir_path, sanitize_filename(url));

        if spa_urls.contains(url) {
            fetch_with_chrome(url, &filename).await;
        } else {
            fetch_with_http(url, &filename).await;
        }
    }

    // ‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå `summary.txt`
    let elapsed_time = start_time.elapsed();
    let summary_content = format!(
        "üåê Total URLs: {}\nSPA URLs: {}\nSSR URLs: {}\n‚è≥ Total Crawl Time: {:.2?}",
        all_urls.len(),
        spa_urls.len(),
        ssr_urls.len(),
        elapsed_time
    );

    write("all-markdown/summary.txt", summary_content).expect("Failed to write summary file");

    println!("üéâ Web Crawling Completed!");
}

// ‚úÖ ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ Sitemap ‡∏´‡∏•‡∏≤‡∏¢‡∏ß‡∏¥‡∏ò‡∏µ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô Googlebot
async fn discover_sitemaps(domain: &str) -> Vec<String> {
    let mut sitemaps = Vec::new();

    // ‚úÖ 1. ‡∏î‡∏∂‡∏á Sitemap ‡∏à‡∏≤‡∏Å robots.txt
    let robots_sitemaps = fetch_sitemaps_from_robots(domain).await;
    sitemaps.extend(robots_sitemaps);

    // ‚úÖ 2. ‡∏•‡∏≠‡∏á‡πÄ‡∏î‡∏≤ URL ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ
    let possible_sitemaps = vec![
        format!("{}/sitemap.xml", domain),
        format!("{}/sitemap_index.xml", domain),
        format!("{}/sitemaps.xml", domain),
    ];
    for sitemap in &possible_sitemaps {
        if let Ok(_) = reqwest::get(sitemap).await {
            sitemaps.push(sitemap.clone());
        }
    }

    // ‚úÖ 3. ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ Sitemap ‡∏à‡∏≤‡∏Å `<head>` ‡∏Ç‡∏≠‡∏á‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå
    if let Ok(head_sitemap) = fetch_sitemap_from_html_head(domain).await {
        sitemaps.extend(head_sitemap);
    }

    // ‚úÖ 4. ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ Sitemap ‡∏à‡∏≤‡∏Å‡∏•‡∏¥‡∏á‡∏Å‡πå‡πÉ‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏£‡∏Å
    if let Ok(link_sitemap) = fetch_sitemap_from_links(domain).await {
        sitemaps.extend(link_sitemap);
    }

    sitemaps
}

// ‚úÖ ‡∏î‡∏∂‡∏á Sitemap ‡∏à‡∏≤‡∏Å robots.txt
async fn fetch_sitemaps_from_robots(domain: &str) -> Vec<String> {
    let robots_url = format!("{}/robots.txt", domain);
    let response = reqwest::get(&robots_url).await;
    match response {
        Ok(resp) => {
            let text = resp.text().await.unwrap_or_default();
            text.lines()
                .filter(|line| line.starts_with("Sitemap:"))
                .map(|line| line.replace("Sitemap: ", "").trim().to_string())
                .collect()
        }
        Err(_) => vec![],
    }
}

// ‚úÖ ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ Sitemap ‡∏à‡∏≤‡∏Å `<head>` ‡∏Ç‡∏≠‡∏á HTML
async fn fetch_sitemap_from_html_head(domain: &str) -> Result<Vec<String>, Error> {
    let mut sitemaps = Vec::new();
    let response = reqwest::get(domain).await?;
    let text = response.text().await?;
    let document = Html::parse_document(&text);
    let selector = Selector::parse("link[rel='sitemap']").unwrap();

    for element in document.select(&selector) {
        if let Some(href) = element.value().attr("href") {
            sitemaps.push(format!("{}/{}", domain, href));
        }
    }

    Ok(sitemaps)
}

// ‚úÖ ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ Sitemap ‡∏à‡∏≤‡∏Å‡∏•‡∏¥‡∏á‡∏Å‡πå `<a href>` ‡πÉ‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏£‡∏Å
async fn fetch_sitemap_from_links(domain: &str) -> Result<Vec<String>, Error> {
    let mut sitemaps = Vec::new();
    let response = reqwest::get(domain).await?;
    let text = response.text().await?;
    let document = Html::parse_document(&text);
    let selector = Selector::parse("a[href*='sitemap']").unwrap();

    for element in document.select(&selector) {
        if let Some(href) = element.value().attr("href") {
            sitemaps.push(format!("{}/{}", domain, href));
        }
    }

    Ok(sitemaps)
}

// ‚úÖ ‡πÇ‡∏´‡∏•‡∏î Sitemap ‡∏î‡∏¥‡∏ö
async fn fetch_sitemap_raw(url: &str) -> Result<String, Error> {
    let response = reqwest::get(url).await?;
    response.text().await
}

// ‚úÖ ‡∏î‡∏∂‡∏á Sitemap ‡∏¢‡πà‡∏≠‡∏¢‡∏à‡∏≤‡∏Å `sitemap index.xml`
fn extract_sitemap_links(xml: &str) -> Vec<String> {
    let document = Html::parse_document(xml);
    let selector = Selector::parse("loc").unwrap();
    document
        .select(&selector)
        .map(|node| node.text().collect::<String>())
        .collect()
}

// ‚úÖ ‡∏î‡∏∂‡∏á URLs ‡∏à‡∏≤‡∏Å Sitemap
fn extract_urls_from_sitemap(xml: &str) -> Vec<String> {
    let document = Html::parse_document(xml);
    let selector = Selector::parse("url loc").unwrap();
    document
        .select(&selector)
        .map(|node| node.text().collect::<String>())
        .collect()
}

// ‚úÖ ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô SPA ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
async fn is_spa(url: &str) -> bool {
    let response = reqwest::get(url).await;
    match response {
        Ok(resp) => {
            let body = resp.text().await.unwrap_or_default();
            body.contains("window.__NUXT__") || body.contains("data-reactroot")
        }
        Err(_) => false,
    }
}

// ‚úÖ Fetch ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å SSR (HTTP Request)
async fn fetch_with_http(url: &str, filename: &str) {
    let response = reqwest::get(url).await;
    if let Ok(resp) = response {
        let html = resp.text().await.unwrap_or_default();
        let markdown = html_to_markdown(&html);
        write(filename, markdown).expect("Failed to write file");
    }
}

// ‚úÖ Fetch ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å SPA (Chrome Headless)
async fn fetch_with_chrome(url: &str, filename: &str) {
    let mut website: Website = Website::new(url).with_caching(true).build().unwrap();
    website.crawl().await;

    if let Some(pages) = website.get_pages() {
        if let Some(page) = pages.iter().find(|p| p.get_url() == url) {
            let html = page.get_html();
            let markdown = html_to_markdown(&html);
            write(filename, markdown).expect("Failed to write file");
        }
    }
}

// ‚úÖ ‡πÅ‡∏õ‡∏•‡∏á HTML ‚Üí Markdown
fn html_to_markdown(html: &str) -> String {
    let document = Html::parse_document(html);
    let selector = Selector::parse("body").unwrap();
    document
        .select(&selector)
        .next()
        .map(|body| body.text().collect::<Vec<_>>().join("\n"))
        .unwrap_or_else(|| "‚ö†Ô∏è No content extracted".to_string())
}

// ‚úÖ ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢
fn sanitize_filename(url: &str) -> String {
    url.replace("https://", "")
        .replace("http://", "")
        .replace("/", "_")
        .replace(":", "_")
}

// ‚úÖ ‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏° URL ‡πÄ‡∏õ‡πá‡∏ô Directory (‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡∏≠‡∏á URL)
fn categorize_url(url: &str) -> String {
    if url.contains("/blog") {
        "blogs".to_string()
    } else if url.contains("/product") {
        "products".to_string()
    } else if url.contains("/news") {
        "news".to_string()
    } else {
        "others".to_string()
    }
}
